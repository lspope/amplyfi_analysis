{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39764bitamplyfienvconda51d3db5d6b6542ce81a8550085ba6795",
   "display_name": "Python 3.9.7 64-bit ('amplyfi_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMPLYFI Article Analysis\n",
    "## Using the OSEMN Process to conduct Exploratory Data Analysis\n",
    "\n",
    "### OSEMN stands for Obtain, Scrub, Explore, Model, and iNterpret. All stages of the process are used for Machine Learning projects. \n",
    "\n",
    "### This is an Exploratory Data Analysis project and uses only the Obtain, Scrub, Explore steps to discover insights about the Environmental, Societal, and Governance (ESG) Article dataset.\n",
    "\n",
    "More details on OSEMN found here: https://machinelearningmastery.com/how-to-work-through-a-problem-like-a-data-scientist/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mongoActions\n",
    "import nlpActions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data\n",
    "### Original data was a ~10,000 json files containing articles. The following steps were taken to obtain the data used in this analysis:\n",
    "- Articles were ingested into a MongoDB by a process that also obtained metatdata for each article (wordCount, lists of Entities, entityMentions, standardized source name). See populateMongo.py and nlpActions.py for this code.\n",
    "- Articles to be analysed are retrieved from the document store (MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arts_df = pd.DataFrame(mongoActions.get_articles_no_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and global variables\n",
    "entity_labels = [\"person\", \"norp\", \"fac\", \"org\", \"gpe\", \"loc\", \"product\", \"event\", \"law\"]\n",
    "\n",
    "def calc_most_common(entity_values, most_common_n):\n",
    "    # getting the counts for entity values\n",
    "    entity_dict = {}\n",
    "    for e_list in entity_values:\n",
    "        for e in e_list:\n",
    "            if e in entity_dict:\n",
    "                entity_dict[e] = entity_dict[e] + 1\n",
    "            else:\n",
    "                entity_dict[e] = 1\n",
    "    most_common_entities = dict(Counter(entity_dict).most_common(most_common_n))\n",
    "    return most_common_entities\n",
    "\n",
    "def get_source_most_common_entities(source, entity_label, top_n):\n",
    "    most_common = None\n",
    "    src_df = all_arts_df[all_arts_df[\"sourceStandardized\"] == source]\n",
    "    src_ents = src_df[entity_label]\n",
    "    if not src_ents.empty:\n",
    "        src_ents_values = src_ents.values\n",
    "        most_common = calc_most_common(src_ents_values, top_n)\n",
    "    return most_common\n",
    "\n",
    "def get_year_most_common_entities(pubYear, entity_label, top_n):\n",
    "    year_most_common = None\n",
    "    year_df = all_arts_df[all_arts_df[\"publishYear\"] == pubYear]\n",
    "    year_ents = year_df[entity_label]\n",
    "    if not year_ents.empty:\n",
    "        year_ents_values = year_ents.values\n",
    "        year_most_common = calc_most_common(year_ents_values, top_n)\n",
    "    return year_most_common\n",
    "\n",
    "def plot_most_common_entities(top_entities, cols, facet_name, color, figsize=(6,8)):\n",
    "        plt_df = pd.DataFrame(list(top_entities.items()), columns=cols)\n",
    "        entity_label = cols[0]\n",
    "        fig = plt_df.plot.barh(legend=False, color=color, figsize=figsize)\n",
    "        plt.title(f\"Top {plt_df.shape[0]} {entity_label} in {facet_name}\")\n",
    "        plt.ylabel(entity_label)\n",
    "        plt.yticks(range(plt_df.shape[0]), plt_df[entity_label].values)\n",
    "        plt.xlabel(f\"Total Articles Mentioning Entity\")\n",
    "        plt.show.legend=False\n",
    "        plt.show()\n",
    "\n",
    "# set the plot theme to seaborn\n",
    "sns.set_theme()\n",
    "# set the face color globally for all axes objects\n",
    "plt.rcParams.update({'axes.facecolor':'whitesmoke'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrub\n",
    "### Even though some preprocesing was done, some data cleaning/scrubbing is needed\n",
    "- Drop Duplicates\n",
    "- Confirm data types are as expected\n",
    "- No need for null checks as the data ingestion to MongoDB addressed this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some duplicate articles were added by mistake at ingest into MongoDB. Remove them.\n",
    "all_arts_df.drop_duplicates(subset=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataframe\n",
    "all_arts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects types for every attribute except for publishDate are acceptable.  \n",
    "# Change publishDate to datetime in case we'd like to use this in exploration\n",
    "all_arts_df[\"publishDate\"] = pd.to_datetime(all_arts_df[\"publishDate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "### Get some basic information about the entire article dataset\n",
    "- Total Articles\n",
    "- Total Sources\n",
    "- Word Count Stats\n",
    "- Entity Mention Stats\n",
    "- Publish Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic stats for all articles\n",
    "article_count = all_arts_df.shape[0]\n",
    "source_count = len(all_arts_df[\"sourceStandardized\"].unique())\n",
    "min_pubdate = all_arts_df[\"publishDate\"].min()\n",
    "max_pubdate = all_arts_df[\"publishDate\"].max()\n",
    "min_wc = all_arts_df[\"wordCount\"].min()\n",
    "max_wc = all_arts_df[\"wordCount\"].max()\n",
    "avg_wc = round(all_arts_df[\"wordCount\"].mean())\n",
    "med_wc = round(all_arts_df[\"wordCount\"].median())\n",
    "min_em = all_arts_df[\"entityMentions\"].min()\n",
    "max_em = all_arts_df[\"entityMentions\"].max()\n",
    "avg_em = round(all_arts_df[\"entityMentions\"].mean())\n",
    "med_em = round(all_arts_df[\"entityMentions\"].median())\n",
    "print(f\"Basic Stats: {article_count} Total Articles, {source_count} Total Sources\\n\")\n",
    "print(f\"Word Count Stats: {min_wc} Min, {max_wc} Max, {avg_wc} Avg, {med_wc} Median\\n\")\n",
    "print(f\"Entity Mention Stats: {min_em} Min, {max_em} Max, {avg_em} Avg, {med_em} Median\\n\")\n",
    "print(f\"Publication date range is {min_pubdate} through {max_pubdate}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "#### What are the article counts for each Year in the Publish Date range?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_arts_year_gb = all_arts_df.groupby(\"publishYear\")[\"id\"].count() \n",
    "\n",
    "xticklabels = []\n",
    "i = 0\n",
    "for i in range(len(all_arts_year_gb.values)):\n",
    "    ticklabel = str(all_arts_year_gb.index[i]) + \" \\n(\" + str(all_arts_year_gb.values[i]) + \")\"\n",
    "    xticklabels.append(ticklabel)\n",
    "    i+=1\n",
    "\n",
    "ax = all_arts_year_gb.plot(kind=\"bar\", rot=0, figsize=(6,6), color=\"lightblue\")\n",
    "ax.set_xticklabels(xticklabels)\n",
    "\n",
    "plt.title(\"Article Count by Year\")\n",
    "plt.xlabel(\"Published Year\")\n",
    "plt.ylabel(\"Count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "### Now that we've discovered some basic information, let's pose 1-2 Analysis Questions and explore the data for insights into these specific areas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Point: The Year with the most Articles\n",
    "### 2019 had the most articles (5184 in total).\n",
    "\n",
    "### What might be interesting about these ESG articles. Exploring PERSON and EVENT entities mentioned in these articles might provide context/insight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "### Politicians\\World Leaders dominated PERSON entity mentions in the 2019 Articles in the dataset.\n",
    "- This is not surprising since the articles were taken from AMPLYFI's harvest engine in relation to Environmental, Societal and Governance (ESG) terms.\n",
    "\n",
    "### The EVENT entity mentions also appear to be in alignment with an ESG search\n",
    "- Sporting Events: Olympic Games, World Cup, U.S. Open, Super Bowl\n",
    "- Annual Events: Cyber Monday, Black Friday, New Year, Chinese New Year, Holy Week, Independence Day\n",
    "- Historic events: the Financial Crisis, the Civil War, Great Depression, the Korean War, Industrial Revolution, the Vietnam War, the Iraq War, Word War I, Watergate, Holocaust, the Cold War, World War II\n",
    "- Climate and Science Events: Hurricanes Irma and Dorian and Maria and Hugo, the Paris Climate Accord, Horizon 2020, United Nations Climate Action Summit \n",
    "\n",
    "### Why is WW II mentioned so often in 2019?\n",
    "- A short Google research session reveals that the 75th Anniversary of D-Day occured in June and the 80th Anniversary of the start of WW II was in September.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What PEERSON entities were detected in the 2019 ESG articles?\n",
    "year_common_ents_per = get_year_most_common_entities(2019, \"person\", 50)\n",
    "plot_most_common_entities(year_common_ents_per, [\"PERSON\", \"Article Count\"], \"2019\", \"lightblue\", (10,12))\n",
    "# Politicians\\World Leaders made up the  majority of Person entity mentions in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What EVENT entities were detected in the 2019 ESG articles?\n",
    "year_common_ents_per = get_year_most_common_entities(2019, \"event\", 50)\n",
    "plot_most_common_entities(year_common_ents_per, [\"EVENT\", \"Article Count\"], \"2019\", \"plum\", (10,12))\n",
    "\n",
    "\n",
    "# These Events do appear relevant to an ESG search \n",
    "#\n",
    "# Sporting events: Olympic Games, World Cup, U.S. Open, Super Bowl\n",
    "#\n",
    "# Annual Events: Cyber Monday, Black Friday, New Year, Chinese New Year, Holy Week, Independence Day\n",
    "\n",
    "# Historic events: he Financial Crisis, the Civil War, Great Depression, the Korean War, Industrial Revolution, \n",
    "# the Vietnam War, the Iraq War, Word War I, Watergate, Holocaust, the Cold War, World War II\n",
    "# \n",
    "# Climate and Science Events: Hurricanes Irma and Dorian and Maria and Hugo, the Paris Climate Accord, \n",
    "# Horizon 2020, United Nations Climate Action Summit \n",
    "#\n",
    "# Why is WW II mentioned so often in 2019?\n",
    "# Quick Google research shows that the 75th Anniversary of D-Day occured in June and the\n",
    "# 80th Anniversary of the start of WW II was in September."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Insights\n",
    "### The current Entity Extraction tool (SpaCy) could be improved or replaced with a better version.\n",
    "- Instagram and Brexit should not be considered Person entities\n",
    "\n",
    "### An Entity Resolution tool would be helpful for future analysis.\n",
    "- If analysis are searching for Entities using \"keyword\" searches, they should be aware of the different ways Entities can be mentioned. \n",
    "- For example multiple PERSON representations for Joe Biden exist (Biden, Joe, and Joe Biden) so being aware of variations could help analysts create a better keyword search term list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Point 2:  Sources\n",
    "### How might we analyse and compare all the Sources? Comparing by article count per source seems to be a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all articles by source\n",
    "allsources_gb = all_arts_df.groupby(\"sourceStandardized\")\n",
    "allsources_art_counts = allsources_gb[\"id\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Min/Max/Average/Median Article Counts by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_min_art_count = allsources_art_counts.min()\n",
    "sources_max_art_count = allsources_art_counts.max()\n",
    "sources_avg_art_count  = round(allsources_art_counts.mean())\n",
    "sources_median_art_count = round(allsources_art_counts.median())\n",
    "print(\"Source article count stats: \\n\")\n",
    "print(f\"Min article count: {sources_min_art_count} \\n\")\n",
    "print(f\"Max article count: {sources_max_art_count} \\n\")\n",
    "print(f\"Average article count:{sources_avg_art_count} Avg\\n\")\n",
    "print(f\"Median article count: {sources_median_art_count}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source article counts are right skewed with strong outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.boxplot(allsources_art_counts, vert=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'd like to examine the Sources that contain the most articles. How might we set an informed threshold for article count?\n",
    "### One approach is to calculate the percentiles (see below). 99% of the Sources had an Article Count of 107 or less. Let's consider the Top 1% (Sources with an Article Count > 107).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_art_count_percentiles = allsources_art_counts.quantile(np.arange(0.01, 1, 0.01))\n",
    "sources_art_count_percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Insight \n",
    "### The Sources in the chart below are the in the Top 1% of this dataset (based on Article Count)\n",
    "- If an organization had to prioritize their Source costs (such as subscriptions/API/access fees), they might want to focus their spending on the Sources with the most ESG articles. \n",
    "- Reuters and Yahoo! News are top ranked. There may be considerable overlap in news stories as they are both general/worldwide news outlets. If analysis time was limited - it might be advantagous to choose one of these two to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_count_sources_ordered = allsources_art_counts.loc[allsources_art_counts>107].sort_values(ascending=True)\n",
    "ax = highest_count_sources_ordered.plot(kind=\"barh\", figsize=(10,12), color=\"lightblue\")\n",
    "plt.title(\"Highest Percentile Sources\")\n",
    "plt.ylabel(\"Source (standarized)\")\n",
    "plt.xlabel(\"Article Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}